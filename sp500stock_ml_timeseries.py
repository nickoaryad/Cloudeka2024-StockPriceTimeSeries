# -*- coding: utf-8 -*-
"""SP500Stock_ML_Timeseries.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JYwhR7yoSfAef5Tek_SI6nrZxaQauE-9

# **Second Project: Developing Machine Learning Model Using Data Time Series**

**Name: Nicko Arya Dharma**   
**Email: nicko.arya.dharma@gmail.com**   
**DicodingID: nickoaryad**

## **1 <font color='yellow'>**|**</font> About the Dataset**

This dataset includes all the historical data of S&P 500 index from 1927 to 2020 with the date, opening price, highest price, lowest price, closing price, adjusted closing price( after all applicable splits and dividend distributions), and the number of shares traded of the day as columns.

The dataset consists of two columns:   
* Date   
* Opening Price   
* Lowest Price   
* Highest Price   
* Closing Price    
* Adjusting Closing Price   
* Volume    

Source:   
https://www.kaggle.com/code/minhhoangngcyber/energy-consumption-prediction-tensorflow-lstm-rnn/input?select=EKPC_hourly.csv

## **2 <font color='yellow'>**|**</font> Importing Libraries**
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import zipfile

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, Dropout
from tensorflow.keras.optimizers import SGD
from tensorflow.keras.losses import Huber
from tensorflow.keras.callbacks import Callback, ReduceLROnPlateau, EarlyStopping
from tensorflow.keras.utils import plot_model
from keras.preprocessing.sequence import TimeseriesGenerator
from keras.models import Sequential, load_model


from sklearn import metrics
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from datetime import datetime, date

import warnings
warnings.filterwarnings("ignore")

"""## **3 <font color='yellow'>**|**</font> Preparing the Dataset**

#### **3.1 <font color='yellow'>**|**</font> Extracting the Dataset**
"""

local_zip = '/SPX.csv.zip'
zip_ref = zipfile.ZipFile(local_zip, 'r')
zip_ref.extractall('/tmp')
zip_ref.close()

"""#### **3.2 <font color='yellow'>**|**</font> Reading the Dataset**"""

data = pd.read_csv('/tmp/SPX.csv', on_bad_lines='skip', encoding='latin1')
data

data.info()

"""#### **3.3 <font color='yellow'>**|**</font> Wrangling the Dataset**"""

# Changing data type
data['Date'] = pd.to_datetime(data['Date'])

# Deleting columns
data = data.drop(columns=['Open', 'High', 'Low', 'Adj Close', 'Volume'], axis=1)

# Renaming column
data.columns = ['Date', 'Price']

data

# Inspecting any NaN data
data.isna().sum()

# Inspecting any null data
data.isnull().sum()

"""#### **3.4 <font color='yellow'>**|**</font> Plotting the Dataset**"""

dates = data['Date'].values
loads  = data['Price'].values

plt.figure(figsize=(15,5))
plt.plot(dates, loads, color='orange')
plt.title(' S&P 500 Index', fontsize=18, fontweight='bold')
plt.xlabel('Date', fontsize=14)
plt.ylabel('Price', fontsize=14)

"""## **4 <font color='yellow'>**|**</font> Preprocessing the Dataset**

#### **4.1 <font color='yellow'>**|**</font> Checking Outliers in the Dataset**
"""

# Checking for any outliers in the dataset
plt.figure(figsize=(15,5))
sns.boxplot(x='Price', data=data)

plt.figure(figsize=(15,5))
sns.kdeplot(x='Price', data=data)

"""#### **4.2 <font color='yellow'>**|**</font> Removing Outliers from the Dataset**"""

# Removing outliers using IQR method
def IQR_outliers(data, column):
   Q1 = data['Price'].quantile(.25)
   Q3 = data['Price'].quantile(.75)
   IQR = Q3 - Q1
   data = data.loc[lambda data: ~((data['Price'] < (Q1 - 1.5 * IQR)) | (data['Price'] > (Q3 + 1.5 * IQR)))]
   return data
data = data.pipe(IQR_outliers, 'Price').pipe(IQR_outliers, 'Price')

data

print(f'The dataframe has {data.shape[0]} records')

plt.figure(figsize=(15,5))
sns.boxplot(x='Price', data=data)

plt.figure(figsize=(15,5))
sns.kdeplot(x='Price', data=data)

"""#### **4.4 <font color='yellow'>**|**</font> Normalizing the Dataset**"""

normalizer = MinMaxScaler()
data['Price'] = normalizer.fit_transform(data['Price'].values.reshape(-1, 1))
data

sns.kdeplot(x='Price', data=data)

"""## **5 <font color='yellow'>**|**</font> Splitting the Dataset**"""

data_train = data.values
X = data_train[:,0:1] #Atribut
Y = data_train[:,1:2] #Label

X

Y

data_train

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, shuffle=False)

print("Train Set     = ", len(X_train))
print("Test Set      =  ", len(X_test))
print("Total Dataset = ", len(data_train))

"""## **6 <font color='yellow'>**|**</font> Developing Model**

#### **6.1 <font color='yellow'>**|**</font> Developing Model Batch**
"""

def windowed_dataset(series, window_size, batch_size, shuffle_buffer):
    series = tf.expand_dims(series, axis=-1)
    ds = tf.data.Dataset.from_tensor_slices(series)
    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)
    ds = ds.flat_map(lambda w: w.batch(window_size + 1))
    ds = ds.shuffle(shuffle_buffer)
    ds = ds.map(lambda w: (w[:-1], w[-1:]))
    return ds.batch(batch_size).prefetch(1)

window_size = 60
batch_size = 64
shuffle_buffer = 1000

Y_train = np.asarray(Y_train).astype(np.float32)
Y_test = np.asarray(Y_test).astype(np.float32)

train_set = windowed_dataset(Y_train, window_size=window_size, batch_size=batch_size, shuffle_buffer=shuffle_buffer)
val_set = windowed_dataset(Y_test, window_size=window_size, batch_size=batch_size, shuffle_buffer=shuffle_buffer)

max = data['Price'].max()
min = data['Price'].min()

print('Max value         = ', max)

print('Min Value         = ', min)

threshold = (max - min) * (10/100)
print('MAE Threshold     = ', threshold)

"""#### **6.2 <font color='yellow'>**|**</font> Sequential Modelling using LSTM**"""

model = Sequential([
  LSTM(units=64, batch_input_shape=(64, 64, 1), return_sequences=True),
  LSTM(64),
  Dense(32, activation="relu"),
  Dropout(0.5),
  Dense(16, activation="relu"),
  Dropout(0.2),
  Dense(1)
])

plot_model(model, show_shapes = True)

optimizer = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9)

lr_schedule = tf.keras.callbacks.LearningRateScheduler(
    lambda epoch: 1e-8 * 10**(epoch / 20))

model.compile(optimizer=optimizer,
              metrics=["mae"],
              loss=tf.keras.losses.Huber())

model.summary()

"""#### **6.3 <font color='yellow'>**|**</font> Defining Callbacks to Control Epochs**"""

# Defining calbacks to stop epoch
class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('mae')< threshold):
      self.model.stop_training = True
      print("\nMAE of the model < 10% of data scale")
callbacks = myCallback()

auto_reduction_LR = ReduceLROnPlateau(
    monitor = 'mae',
    patience = 2, # epoch waiting to decrease LR by factor
    verbose = 1,
    factor = 0.1, #factor for decreasing LR
    min_lr = 1.5e-10 #minimum Learning Rate
)

"""#### **6.4 <font color='yellow'>**|**</font> Training the Model Using Fit Function**"""

num_epochs = 100
history = model.fit(train_set,
                    epochs=num_epochs,
                    validation_data=val_set,
                    verbose=2,
                    callbacks=[callbacks, auto_reduction_LR])

"""#### **6.5 <font color='yellow'>**|**</font> Saving the Model for Deployment**"""

model.save_weights('model_weights.h5')
model.save('model.h5')

"""## **7 <font color='yellow'>**|**</font> Plotting**

#### **7.1 <font color='yellow'>**|**</font> Loss of Training and Validation**
"""

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epochs')
plt.legend(['Training Loss', 'Validation Loss'], loc = 'upper right')
plt.show()

"""#### **7.2 <font color='yellow'>**|**</font> Accuracy of Training and Validation**"""

plt.plot(history.history['mae'])
plt.plot(history.history['val_mae'])
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Training Accuracy', 'Validation Accuracy'], loc='lower right')
plt.show()